Process started at Mon, Dec  9, 2024  9:13:20 AM
Running Python script...
1159250 Price: 232,950
1159094 Price: 30,540
1159169 Price: 12,660
1143783 Price: 2,390
5117379 Price: 109.59 ‡‚í
5113444 Price: 135.68 ‡‚í
Prices saved to C:/Users/Tomer/Documents/GitHub/stock_scrape/prices.csv.
Scraping completed successfully
Process ended at Mon, Dec  9, 2024  9:13:35 AM
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master f122c4d] Updated prices.csv and process log
 1 file changed, 2 insertions(+), 132 deletions(-)
To github.com:tomerzipori/stock_scrape.git
   60c9ec7..f122c4d  master -> master
Process started at Mon, Dec  9, 2024  8:03:46 PM
Running Python script...
1159250 Price: 229,640
1159094 Price: 30,340
1159169 Price: 12,740
1143783 Price: 2,418
5117379 Price: 109.59 ‡‚í
5113444 Price: 135.99 ‡‚í
Prices saved to C:/Users/Tomer/Documents/GitHub/stock_scrape/prices.csv.
Scraping completed successfully
Process ended at Mon, Dec  9, 2024  8:04:02 PM
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master a18d8c2] Updated prices.csv and process log
 2 files changed, 21 insertions(+), 5 deletions(-)
To github.com:tomerzipori/stock_scrape.git
   f122c4d..a18d8c2  master -> master
Process started at Tue, Dec 10, 2024  9:41:46 PM
Running Python script...
C:/Users/Tomer/Documents/GitHub/stock_scrape/stock_scrape.sh: line 27: C:/Users/Tomer/Documents/GitHub/stock_scrape/.venv/bin/python: No such file or directory
Scraping script encountered an error. Check the details below:
Process ended at Tue, Dec 10, 2024  9:41:46 PM
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master e367b13] Updated prices.csv and process log
 1 file changed, 5 insertions(+)
To github.com:tomerzipori/stock_scrape.git
   d3a7ed8..e367b13  master -> master
Process started at Tue Dec 10 21:44:35 JST 2024
Running Python script...
./stock_scrape.sh: line 27: C:/Users/Tomer/Documents/GitHub/stock_scrape/.venv/bin/python: No such file or directory
Scraping script encountered an error. Check the details below:
Process ended at Tue Dec 10 21:44:35 JST 2024
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master 9699533] Updated prices.csv and process log
 2 files changed, 16 insertions(+), 6 deletions(-)
To github.com:tomerzipori/stock_scrape.git
   e367b13..9699533  master -> master
Process started at Tue Dec 10 21:46:59 JST 2024
Running Python script...
bash: C:/Users/Tomer/Documents/GitHub/stock_scrape/.venv/bin/python: No such file or directory
Scraping script encountered an error. Check the details below:
Process ended at Tue Dec 10 21:47:00 JST 2024
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master 761b7b3] Updated prices.csv and process log
 2 files changed, 11 insertions(+), 1 deletion(-)
To github.com:tomerzipori/stock_scrape.git
   9699533..761b7b3  master -> master
Process started at Tue Dec 10 21:47:24 JST 2024
Running Python script...
./stock_scrape.sh: line 27: C:/Users/Tomer/Documents/GitHub/stock_scrape/.venv/bin/python: No such file or directory
Scraping script encountered an error. Check the details below:
Process ended at Tue Dec 10 21:47:24 JST 2024
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master dd6d444] Updated prices.csv and process log
 1 file changed, 10 insertions(+)
To github.com:tomerzipori/stock_scrape.git
   761b7b3..dd6d444  master -> master
Process started at Wed, Dec 11, 2024  6:26:29 PM
Running Python script...
C:/Users/Tomer/Documents/GitHub/stock_scrape/stock_scrape.sh: line 27: C:/Users/Tomer/Documents/GitHub/stock_scrape/.venv/bin/python: No such file or directory
Scraping script encountered an error. Check the details below:
Process ended at Wed, Dec 11, 2024  6:26:29 PM
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master 9b5ed95] Updated prices.csv and process log
 1 file changed, 10 insertions(+)
To github.com:tomerzipori/stock_scrape.git
   dd6d444..9b5ed95  master -> master
Process started at Thu, Dec 12, 2024  8:05:21 PM
Running Python script...
C:/Users/Tomer/Documents/GitHub/stock_scrape/stock_scrape.sh: line 27: C:/Users/Tomer/Documents/GitHub/stock_scrape/.venv/bin/python: No such file or directory
Scraping script encountered an error. Check the details below:
Process ended at Thu, Dec 12, 2024  8:05:21 PM
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master a2ff385] Updated prices.csv and process log
 1 file changed, 10 insertions(+)
To github.com:tomerzipori/stock_scrape.git
   9b5ed95..a2ff385  master -> master
Process started at Thu Dec 12 20:16:11 JST 2024
Running Python script...
./stock_scrape.sh: line 27: C:/Users/Tomer/Documents/GitHub/stock_scrape/.venv/bin/python: No such file or directory
Scraping script encountered an error. Check the details below:
Process ended at Thu Dec 12 20:16:11 JST 2024
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master ba9454f] Updated prices.csv and process log
 1 file changed, 10 insertions(+)
To github.com:tomerzipori/stock_scrape.git
   a2ff385..ba9454f  master -> master
Process started at Sun, Dec 15, 2024  6:21:23 PM
Running Python script...
C:/Users/Tomer/Documents/GitHub/stock_scrape/stock_scrape.sh: line 27: C:/Users/Tomer/Documents/GitHub/stock_scrape/.venv/bin/python: No such file or directory
Scraping script encountered an error. Check the details below:
Process ended at Sun, Dec 15, 2024  6:21:23 PM
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master a74214a] Updated prices.csv and process log
 1 file changed, 5 insertions(+)
To github.com:tomerzipori/stock_scrape.git
   afdec5a..a74214a  master -> master
Process started at Sun Dec 15 18:36:18 JST 2024
Running Python script...
bash: C:/Users/Tomer/Documents/GitHub/stock_scrape/.venv/bin/python: No such file or directory
Scraping script encountered an error. Check the details below:
Process ended at Sun Dec 15 18:36:18 JST 2024
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master e6b0621] Updated prices.csv and process log
 1 file changed, 10 insertions(+)
To github.com:tomerzipori/stock_scrape.git
   a74214a..e6b0621  master -> master
Process started at Sun Dec 15 18:44:35 JST 2024
Running Python script...
./stock_scrape.sh: line 27: C:/Users/Tomer/Documents/GitHub/stock_scrape/.venv/bin/python: No such file or directory
Scraping script encountered an error. Check the details below:
Process ended at Sun Dec 15 18:44:35 JST 2024
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master a0a162f] Updated prices.csv and process log
 1 file changed, 10 insertions(+)
To github.com:tomerzipori/stock_scrape.git
   e6b0621..a0a162f  master -> master
Process started at Sun Dec 15 18:51:51 JST 2024
Running Python script...
./stock_scrape.sh: line 27: /C/Users/Tomer/Documents/GitHub/stock_scrape/.venv/bin/python: No such file or directory
Scraping script encountered an error. Check the details below:
Process ended at Sun Dec 15 18:51:51 JST 2024
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master 8630d2e] Updated prices.csv and process log
 1 file changed, 10 insertions(+)
To github.com:tomerzipori/stock_scrape.git
   a0a162f..8630d2e  master -> master
Process started at Sun Dec 15 18:54:37 JST 2024
Running Python script...
./stock_scrape.sh: line 27: /c/Users/Tomer/Documents/GitHub/stock_scrape/.venv/bin/python: No such file or directory
Scraping script encountered an error. Check the details below:
Process ended at Sun Dec 15 18:54:37 JST 2024
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master 22d6592] Updated prices.csv and process log
 1 file changed, 10 insertions(+)
To github.com:tomerzipori/stock_scrape.git
   8630d2e..22d6592  master -> master
Process started at Sun Dec 15 18:59:53 JST 2024
Running Python script...
1159250 Price: 229,600
1159094 Price: 30,080
1159169 Price: 12,600
1143783 Price: 2,345
5117379 Price: 110.11 ‡‚í
5113444 Price: 136.05 ‡‚í
Prices saved to C:/Users/Tomer/Documents/GitHub/stock_scrape/prices.csv.
Scraping completed successfully
Process ended at Sun Dec 15 19:00:09 JST 2024
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master 3a9f442] Updated prices.csv and process log
 2 files changed, 17 insertions(+), 1 deletion(-)
To github.com:tomerzipori/stock_scrape.git
   22d6592..3a9f442  master -> master
Process started at Mon, Dec 16, 2024  8:27:14 PM
Running Python script...
1159250 Price: 231,210
1159094 Price: 30,050
1159169 Price: 12,600
1143783 Price: 2,403
5117379 Price: ‡‚í
5113444 Price: 136.18 ‡‚í
Prices saved to C:/Users/Tomer/Documents/GitHub/stock_scrape/prices.csv.
Scraping completed successfully
Process ended at Mon, Dec 16, 2024  8:27:30 PM
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master 175b54f] Updated prices.csv and process log
 2 files changed, 21 insertions(+), 5 deletions(-)
To github.com:tomerzipori/stock_scrape.git
   3a9f442..175b54f  master -> master
Process started at Mon, Dec 16, 2024  8:28:07 PM
Running Python script...
1159250 Price: 231,210
1159094 Price: 30,050
1159169 Price: 12,600
1143783 Price: 2,403
5117379 Price: 109.84 ‡‚í
5113444 Price: 136.18 ‡‚í
Prices saved to C:/Users/Tomer/Documents/GitHub/stock_scrape/prices.csv.
Scraping completed successfully
Process ended at Mon, Dec 16, 2024  8:28:22 PM
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master a80dde8] Updated prices.csv and process log
 2 files changed, 17 insertions(+), 1 deletion(-)
To github.com:tomerzipori/stock_scrape.git
   175b54f..a80dde8  master -> master
2024-12-16 20:40:45,325 - ERROR - Error scraping 1159250: HTTPConnectionPool(host='localhost', port=61906): Max retries exceeded with url: /session/ce42fcb9821ee2158d1bedcb835d430a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000239BF194A70>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
Traceback (most recent call last):
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 199, in _new_conn
    sock = connection.create_connection(
        (self._dns_host, self.port),
    ...<2 lines>...
        socket_options=self.socket_options,
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
    ~~~~~~~~~~~~^^^^
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 789, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 495, in _make_request
    conn.request(
    ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<6 lines>...
        enforce_content_length=enforce_content_length,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 441, in request
    self.endheaders()
    ~~~~~~~~~~~~~~~^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1091, in _send_output
    self.send(msg)
    ~~~~~~~~~^^^^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1035, in send
    self.connect()
    ~~~~~~~~~~~~^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 279, in connect
    self.sock = self._new_conn()
                ~~~~~~~~~~~~~~^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 214, in _new_conn
    raise NewConnectionError(
        self, f"Failed to establish a new connection: {e}"
    ) from e
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x00000239BF194A70>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<input>", line 5, in <module>
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 393, in get
    self.execute(Command.GET, {"url": url})
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 382, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\_request_methods.py", line 143, in request
    return self.request_encode_body(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        method, url, fields=fields, headers=headers, **urlopen_kw
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 843, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=61906): Max retries exceeded with url: /session/ce42fcb9821ee2158d1bedcb835d430a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000239BF194A70>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2024-12-16 20:41:05,801 - ERROR - Error scraping 1159094: HTTPConnectionPool(host='localhost', port=61906): Max retries exceeded with url: /session/ce42fcb9821ee2158d1bedcb835d430a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000239BF14A580>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
Traceback (most recent call last):
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 199, in _new_conn
    sock = connection.create_connection(
        (self._dns_host, self.port),
    ...<2 lines>...
        socket_options=self.socket_options,
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
    ~~~~~~~~~~~~^^^^
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 789, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 495, in _make_request
    conn.request(
    ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<6 lines>...
        enforce_content_length=enforce_content_length,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 441, in request
    self.endheaders()
    ~~~~~~~~~~~~~~~^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1091, in _send_output
    self.send(msg)
    ~~~~~~~~~^^^^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1035, in send
    self.connect()
    ~~~~~~~~~~~~^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 279, in connect
    self.sock = self._new_conn()
                ~~~~~~~~~~~~~~^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 214, in _new_conn
    raise NewConnectionError(
        self, f"Failed to establish a new connection: {e}"
    ) from e
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x00000239BF14A580>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<input>", line 5, in <module>
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 393, in get
    self.execute(Command.GET, {"url": url})
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 382, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\_request_methods.py", line 143, in request
    return self.request_encode_body(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        method, url, fields=fields, headers=headers, **urlopen_kw
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 843, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=61906): Max retries exceeded with url: /session/ce42fcb9821ee2158d1bedcb835d430a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000239BF14A580>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2024-12-16 20:41:22,131 - ERROR - Error scraping 1159169: HTTPConnectionPool(host='localhost', port=61906): Max retries exceeded with url: /session/ce42fcb9821ee2158d1bedcb835d430a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000239BF14A9C0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
Traceback (most recent call last):
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 199, in _new_conn
    sock = connection.create_connection(
        (self._dns_host, self.port),
    ...<2 lines>...
        socket_options=self.socket_options,
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
    ~~~~~~~~~~~~^^^^
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 789, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 495, in _make_request
    conn.request(
    ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<6 lines>...
        enforce_content_length=enforce_content_length,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 441, in request
    self.endheaders()
    ~~~~~~~~~~~~~~~^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1091, in _send_output
    self.send(msg)
    ~~~~~~~~~^^^^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1035, in send
    self.connect()
    ~~~~~~~~~~~~^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 279, in connect
    self.sock = self._new_conn()
                ~~~~~~~~~~~~~~^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 214, in _new_conn
    raise NewConnectionError(
        self, f"Failed to establish a new connection: {e}"
    ) from e
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x00000239BF14A9C0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<input>", line 5, in <module>
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 393, in get
    self.execute(Command.GET, {"url": url})
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 382, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\_request_methods.py", line 143, in request
    return self.request_encode_body(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        method, url, fields=fields, headers=headers, **urlopen_kw
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 843, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=61906): Max retries exceeded with url: /session/ce42fcb9821ee2158d1bedcb835d430a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000239BF14A9C0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2024-12-16 20:41:38,417 - ERROR - Error scraping 1143783: HTTPConnectionPool(host='localhost', port=61906): Max retries exceeded with url: /session/ce42fcb9821ee2158d1bedcb835d430a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000239BF14B350>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
Traceback (most recent call last):
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 199, in _new_conn
    sock = connection.create_connection(
        (self._dns_host, self.port),
    ...<2 lines>...
        socket_options=self.socket_options,
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
    ~~~~~~~~~~~~^^^^
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 789, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 495, in _make_request
    conn.request(
    ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<6 lines>...
        enforce_content_length=enforce_content_length,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 441, in request
    self.endheaders()
    ~~~~~~~~~~~~~~~^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1091, in _send_output
    self.send(msg)
    ~~~~~~~~~^^^^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1035, in send
    self.connect()
    ~~~~~~~~~~~~^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 279, in connect
    self.sock = self._new_conn()
                ~~~~~~~~~~~~~~^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 214, in _new_conn
    raise NewConnectionError(
        self, f"Failed to establish a new connection: {e}"
    ) from e
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x00000239BF14B350>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<input>", line 5, in <module>
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 393, in get
    self.execute(Command.GET, {"url": url})
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 382, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\_request_methods.py", line 143, in request
    return self.request_encode_body(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        method, url, fields=fields, headers=headers, **urlopen_kw
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 843, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=61906): Max retries exceeded with url: /session/ce42fcb9821ee2158d1bedcb835d430a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000239BF14B350>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2024-12-16 20:41:54,766 - ERROR - Error scraping 5117379: HTTPConnectionPool(host='localhost', port=61906): Max retries exceeded with url: /session/ce42fcb9821ee2158d1bedcb835d430a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000239BF14BCE0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
Traceback (most recent call last):
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 199, in _new_conn
    sock = connection.create_connection(
        (self._dns_host, self.port),
    ...<2 lines>...
        socket_options=self.socket_options,
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
    ~~~~~~~~~~~~^^^^
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 789, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 495, in _make_request
    conn.request(
    ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<6 lines>...
        enforce_content_length=enforce_content_length,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 441, in request
    self.endheaders()
    ~~~~~~~~~~~~~~~^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1091, in _send_output
    self.send(msg)
    ~~~~~~~~~^^^^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1035, in send
    self.connect()
    ~~~~~~~~~~~~^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 279, in connect
    self.sock = self._new_conn()
                ~~~~~~~~~~~~~~^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 214, in _new_conn
    raise NewConnectionError(
        self, f"Failed to establish a new connection: {e}"
    ) from e
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x00000239BF14BCE0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<input>", line 5, in <module>
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 393, in get
    self.execute(Command.GET, {"url": url})
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 382, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\_request_methods.py", line 143, in request
    return self.request_encode_body(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        method, url, fields=fields, headers=headers, **urlopen_kw
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 843, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=61906): Max retries exceeded with url: /session/ce42fcb9821ee2158d1bedcb835d430a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000239BF14BCE0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2024-12-16 20:42:11,090 - ERROR - Error scraping 5113444: HTTPConnectionPool(host='localhost', port=61906): Max retries exceeded with url: /session/ce42fcb9821ee2158d1bedcb835d430a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000239BF2A46B0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
Traceback (most recent call last):
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 199, in _new_conn
    sock = connection.create_connection(
        (self._dns_host, self.port),
    ...<2 lines>...
        socket_options=self.socket_options,
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    raise err
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
    ~~~~~~~~~~~~^^^^
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 789, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 495, in _make_request
    conn.request(
    ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<6 lines>...
        enforce_content_length=enforce_content_length,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 441, in request
    self.endheaders()
    ~~~~~~~~~~~~~~~^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1091, in _send_output
    self.send(msg)
    ~~~~~~~~~^^^^^
  File "C:\Users\Tomer\AppData\Local\Programs\Python\Python313\Lib\http\client.py", line 1035, in send
    self.connect()
    ~~~~~~~~~~~~^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 279, in connect
    self.sock = self._new_conn()
                ~~~~~~~~~~~~~~^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connection.py", line 214, in _new_conn
    raise NewConnectionError(
        self, f"Failed to establish a new connection: {e}"
    ) from e
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x00000239BF2A46B0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<input>", line 5, in <module>
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 393, in get
    self.execute(Command.GET, {"url": url})
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\webdriver.py", line 382, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\_request_methods.py", line 143, in request
    return self.request_encode_body(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        method, url, fields=fields, headers=headers, **urlopen_kw
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 873, in urlopen
    return self.urlopen(
           ~~~~~~~~~~~~^
        method,
        ^^^^^^^
    ...<13 lines>...
        **response_kw,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\connectionpool.py", line 843, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "C:\Users\Tomer\Documents\GitHub\stock_scrape\.venv\Lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=61906): Max retries exceeded with url: /session/ce42fcb9821ee2158d1bedcb835d430a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000239BF2A46B0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
Process started at Mon, Dec 16, 2024  8:55:06 PM
Running Python script...
1159250 Price: 231,210
1159094 Price: 30,050
1159169 Price: 12,600
1143783 Price: 2,403
5117379 Price: 109.84 ‡‚í
5113444 Price: 136.18 ‡‚í
Prices saved to C:/Users/Tomer/Documents/GitHub/stock_scrape/prices.csv.
Scraping completed successfully
Process ended at Mon, Dec 16, 2024  8:55:22 PM
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master 5c61f5a] Updated prices.csv and process log
 2 files changed, 732 insertions(+), 2 deletions(-)
To github.com:tomerzipori/stock_scrape.git
   a80dde8..5c61f5a  master -> master
Process started at Tue, Dec 17, 2024  9:04:35 PM
Running Python script...
1159250 Price: 230,260
1159094 Price: 29,940
1159169 Price: 12,460
1143783 Price: 2,407
5117379 Price: 109.73 ‡‚í
5113444 Price: 136.2 ‡‚í
Prices saved to C:/Users/Tomer/Documents/GitHub/stock_scrape/prices.csv.
Scraping completed successfully
Process ended at Tue, Dec 17, 2024  9:04:51 PM
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master be4d658] Updated prices.csv and process log
 2 files changed, 22 insertions(+), 6 deletions(-)
To github.com:tomerzipori/stock_scrape.git
   5c61f5a..be4d658  master -> master
Process started at Wed, Dec 18, 2024  8:20:22 AM
Running Python script...
1159250 Price: 230,260
1159094 Price: 29,940
1159169 Price: 12,460
1143783 Price: 2,407
5117379 Price: ‡‚í
5113444 Price: 136.2 ‡‚í
Prices saved to C:/Users/Tomer/Documents/GitHub/stock_scrape/prices.csv.
Scraping completed successfully
Process ended at Wed, Dec 18, 2024  8:20:42 AM
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master e555b02] Updated prices.csv and process log
 2 files changed, 17 insertions(+), 1 deletion(-)
To github.com:tomerzipori/stock_scrape.git
   be4d658..e555b02  master -> master
Process started at Wed, Dec 18, 2024  2:22:20 PM
Running Python script...
1159250 Price: 230,470
1159094 Price: 29,890
1159169 Price: 12,540
1143783 Price: 2,408
5117379 Price: ‡‚í
5113444 Price: 136.2 ‡‚í
Prices saved to C:/Users/Tomer/Documents/GitHub/stock_scrape/prices.csv.
Scraping completed successfully
Process ended at Wed, Dec 18, 2024  2:22:39 PM
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master 338e81c] Updated prices.csv and process log
 2 files changed, 20 insertions(+), 4 deletions(-)
To github.com:tomerzipori/stock_scrape.git
   e555b02..338e81c  master -> master
Process started at Fri, Dec 20, 2024  9:17:08 AM
Running Python script...
1159250 Price: 227,490
1159094 Price: 29,600
1159169 Price: 12,530
1143783 Price: 2,381
5117379 Price: ‡‚í
5113444 Price: 136.18 ‡‚í
Prices saved to C:/Users/Tomer/Documents/GitHub/stock_scrape/prices.csv.
Scraping completed successfully
Process ended at Fri, Dec 20, 2024  9:17:25 AM
warning: in the working copy of 'process_log.txt', LF will be replaced by CRLF the next time Git touches it
[master aa64819] Updated prices.csv and process log
 2 files changed, 21 insertions(+), 5 deletions(-)
To github.com:tomerzipori/stock_scrape.git
   338e81c..aa64819  master -> master
Process started at Fri Dec 20 13:32:06 UTC 2024
Running Python script...
1159250 Price: 227,490
1159094 Price: 29,600
1159169 Price: 12,530
1143783 Price: 2,381
5117379 Price: 109.59 ◊ê◊í‚Äô
5113444 Price: 135.88 ◊ê◊í‚Äô
Prices saved to /app/prices.csv.
Scraping completed successfully
Process ended at Fri Dec 20 13:32:15 UTC 2024
Process started at Fri Dec 20 14:19:37 UTC 2024
Running Python script...
2024-12-20 14:22:43,705 - ERROR - Error scraping 1159250: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 536, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connection.py", line 507, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/http/client.py", line 1395, in getresponse
    response.begin()
  File "/usr/local/lib/python3.11/http/client.py", line 325, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/http/client.py", line 286, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 718, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/scrape.py", line 57, in <module>
    driver.get(url[0])
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 393, in get
    self.execute(Command.GET, {"url": url})
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 382, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/util/util.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 538, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 369, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
2024-12-20 14:24:46,577 - ERROR - Error scraping 1159094: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 536, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connection.py", line 507, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/http/client.py", line 1395, in getresponse
    response.begin()
  File "/usr/local/lib/python3.11/http/client.py", line 325, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/http/client.py", line 286, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 718, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/scrape.py", line 57, in <module>
    driver.get(url[0])
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 393, in get
    self.execute(Command.GET, {"url": url})
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 382, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/util/util.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 538, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 369, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
2024-12-20 14:26:58,888 - ERROR - Error scraping 1159169: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 536, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connection.py", line 507, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/http/client.py", line 1395, in getresponse
    response.begin()
  File "/usr/local/lib/python3.11/http/client.py", line 325, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/http/client.py", line 286, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 718, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/scrape.py", line 57, in <module>
    driver.get(url[0])
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 393, in get
    self.execute(Command.GET, {"url": url})
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 382, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/util/util.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 538, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 369, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
2024-12-20 14:29:06,093 - ERROR - Error scraping 1143783: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 536, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connection.py", line 507, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/http/client.py", line 1395, in getresponse
    response.begin()
  File "/usr/local/lib/python3.11/http/client.py", line 325, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/http/client.py", line 286, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 718, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/scrape.py", line 57, in <module>
    driver.get(url[0])
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 393, in get
    self.execute(Command.GET, {"url": url})
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 382, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/util/util.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 538, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 369, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
2024-12-20 14:31:17,317 - ERROR - Error scraping 5117379: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 536, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connection.py", line 507, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/http/client.py", line 1395, in getresponse
    response.begin()
  File "/usr/local/lib/python3.11/http/client.py", line 325, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/http/client.py", line 286, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 718, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/scrape.py", line 57, in <module>
    driver.get(url[0])
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 393, in get
    self.execute(Command.GET, {"url": url})
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 382, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/util/util.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 538, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 369, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
2024-12-20 14:33:24,286 - ERROR - Error scraping 5113444: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 536, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connection.py", line 507, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/http/client.py", line 1395, in getresponse
    response.begin()
  File "/usr/local/lib/python3.11/http/client.py", line 325, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/http/client.py", line 286, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/socket.py", line 718, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/scrape.py", line 57, in <module>
    driver.get(url[0])
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 393, in get
    self.execute(Command.GET, {"url": url})
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 382, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/util/util.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 538, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 369, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
Error scraping 1159250: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
Error scraping 1159094: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
Error scraping 1159169: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
Error scraping 1143783: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
Error scraping 5117379: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
Error scraping 5113444: HTTPConnectionPool(host='localhost', port=42169): Read timed out. (read timeout=120)
Prices saved to /app/prices.csv.
Scraping completed successfully
Process ended at Fri Dec 20 14:42:25 UTC 2024
